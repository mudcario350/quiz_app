# Variable Questions Implementation Guide

## Overview
The `source_app.py` has been successfully updated to support **1-25 questions per assignment** instead of the fixed 3 questions. The system now dynamically adapts to however many questions are filled in the Google Sheets for each assignment.

## Key Changes

### 1. Google Sheets Schema Updates

#### Assignments Sheet
- **Old Schema**: `date`, `assignment_id`, `Question1`, `Question2`, `Question3`, `GradingPrompt`, `ConversationPrompt`
- **New Schema**: `date`, `assignment_id`, `Question1` through `Question25`, `GradingPrompt`, `ConversationPrompt`
- **New Method**: `get_active_questions(assignment_id)` - Returns only filled questions

#### Student Answers Sheet
- **Old Schema**: `execution_id`, `assignment_id`, `student_id`, `q1_answer`, `q2_answer`, `q3_answer`, `timestamp`
- **New Schema**: `execution_id`, `assignment_id`, `student_id`, `q1_answer` through `q25_answer`, `timestamp`

#### Feedback+Grading Sheet
- **Old Schema**: `execution_id`, `assignment_id`, `student_id`, `feedback1`, `feedback2`, `feedback3`, `score1`, `score2`, `score3`, `timestamp`
- **New Schema**: `execution_id`, `assignment_id`, `student_id`, `feedback1`, `score1`, ... `feedback25`, `score25`, `timestamp`

#### Feedback Evaluation Sheet
- **Old Schema**: Similar to grading but with `new_feedback` and `new_score` prefixes
- **New Schema**: Extended to support 25 questions

### 2. Core Application Changes

#### AssignmentsSheet Class
```python
def get_active_questions(self, assignment_id: str) -> dict[str, str]:
    """Get only the filled questions for a given assignment."""
    # Returns: {"q1": "Question text...", "q5": "Question text...", ...}
    # Only includes questions that have content
```

#### Memory System
- `AssignmentState`: Updated to support variable number of questions in all dictionaries
- `AssignmentMemoryManager`: `initialize_assignment_session` now dynamically creates state based on actual questions

#### Grading System
- **Parallel Grading**: `run_grading_streaming` now creates N parallel threads (max 10 concurrent)
- Dynamically determines which questions to grade based on submitted answers
- Uses ThreadPoolExecutor with worker count = min(num_questions, 10)

#### UI Rendering
- Questions are rendered dynamically based on `active_questions` dictionary
- All validation checks now loop through active questions
- Completion logic checks all active question scores
- Retry section dynamically renders all active questions

### 3. Key Functions Updated

#### `load_questions(aid: str)`
- Now returns `active_questions` dictionary containing only filled questions
- Adds `num_questions` count to the record

#### `record_answers(exec_id, sid, aid, answers)`
- Records all 25 answer columns (empty for unused questions)

#### `run_grading_streaming(exec_id, sid, aid, answers)`
- Determines active questions from submitted answers
- Creates parallel grading threads for each question
- Max 10 concurrent API calls (configurable)

#### Main UI Loop
- Dynamically renders questions: `for q_key, q_text in active_questions.items()`
- Validation checks all active questions
- Feedback display adapts to number of questions

## How to Use

### Setting Up Google Sheets

1. **Update Assignments Sheet**:
   ```
   | date | assignment_id | Question1 | Question2 | ... | Question25 | GradingPrompt | ConversationPrompt |
   ```
   - Fill in as many questions as you want (1-25)
   - Leave unused question columns empty
   - Example with 5 questions: Fill Question1 through Question5, leave Question6-Question25 empty

2. **Example Assignments**:
   
   **Assignment A (1 question)**:
   - Question1: "What is AI?"
   - Question2-25: (empty)
   
   **Assignment B (3 questions)**:
   - Question1: "Define machine learning"
   - Question2: "Explain neural networks"
   - Question3: "What is deep learning?"
   - Question4-25: (empty)
   
   **Assignment C (10 questions)**:
   - Question1-10: Fill with your questions
   - Question11-25: (empty)

### Testing Different Question Counts

To test the system with various question counts:

1. **Test with 1 Question**:
   - Create assignment with only Question1 filled
   - Submit and verify grading works with single question

2. **Test with 3 Questions** (backward compatibility):
   - Fill Question1, Question2, Question3
   - Should work exactly like the old system

3. **Test with 10 Questions**:
   - Fill Question1 through Question10
   - Verify parallel grading completes successfully
   - Check feedback displays for all 10 questions

4. **Test with 25 Questions** (maximum):
   - Fill all Question1 through Question25
   - Verify UI renders all questions
   - Check parallel grading (will use 10 concurrent threads)

## Performance Considerations

### Parallel Grading
- **Max Concurrent Threads**: 10 (configurable in code)
- **Timeout per Question**: 60 seconds
- **Total Grading Time**: Approximately same as longest question + overhead

### Example Timing
- 3 questions: ~5-8 seconds (same as before)
- 10 questions: ~8-12 seconds (with 10 parallel threads)
- 25 questions: ~12-20 seconds (with 10 parallel threads, 3 batches)

## Backward Compatibility

‚úÖ **Fully Backward Compatible**: Existing 3-question assignments will continue to work without any changes.

The system automatically detects how many questions are filled and adapts accordingly.

## Debug Output

The system now includes enhanced debug logging:
```
[DEBUG] Loaded 5 active questions for assignment A001
[DEBUG] Question keys: ['q1', 'q2', 'q3', 'q4', 'q5']
[DEBUG] Grading 5 questions: [1, 2, 3, 4, 5]
[BENCHMARK] All 5 API calls submitted at 0.123s
[BENCHMARK] Q1 API call completed at 3.456s
[BENCHMARK] Q2 API call completed at 3.789s
...
[BENCHMARK] Total parallel grading time: 4.567s
[BENCHMARK] Average time per question: 0.913s
[BENCHMARK] Questions completed: 5/5
```

## UI Changes

### Header
- Now shows: "üìù Round 1 (5 Questions)" instead of just "üìù Round 1"

### Completion Message
- **All passed**: "üéâ You have successfully completed this assignment! All 5 questions passed!"
- **Partial**: "You need scores of 8 or higher on all questions to complete this assignment. (3/5 passed)"

### Question Display
- Questions are numbered based on their actual position (Q1, Q2, Q3... Q25)
- If you use Question1, Question5, and Question10 only, they display as Q1, Q5, Q10

## Code Structure

### Key Data Structures

```python
# Active questions dictionary
active_questions = {
    'q1': 'What is AI?',
    'q5': 'Define ML?',
    'q10': 'Explain neural networks?'
}

# Answers dictionary (matches question keys)
answers = {
    'q1': 'Student answer for Q1...',
    'q5': 'Student answer for Q5...',
    'q10': 'Student answer for Q10...'
}

# Grading result
grade_res = {
    'execution_id': '...',
    'assignment_id': '...',
    'student_id': '...',
    'score1': 8,
    'feedback1': 'Good answer...',
    'score5': 9,
    'feedback5': 'Excellent...',
    'score10': 7,
    'feedback10': 'Needs improvement...',
    # Other scores/feedback are 0/empty
}
```

## Next Steps for Testing

1. ‚úÖ Update your Google Sheets to include 25 question columns
2. ‚úÖ Create test assignments with varying question counts (1, 3, 5, 10, 25)
3. ‚è≥ Test submission and grading with different question counts
4. ‚è≥ Verify feedback display for all questions
5. ‚è≥ Test retry functionality with various question counts
6. ‚è≥ Test session restoration with different question counts

## Notes

- The system uses question numbers from the Google Sheet columns (Question1 = q1, Question5 = q5)
- Empty question columns are automatically skipped
- All 25 columns are written to sheets (with empty values for unused questions)
- Memory system maintains separate context for each question number
- Conversation system has access to context from all active questions

## Troubleshooting

### Issue: "No questions found for this assignment"
- **Cause**: All Question1-Question25 columns are empty
- **Solution**: Fill at least one question column

### Issue: Grading takes longer with many questions
- **Cause**: More API calls needed
- **Solution**: Adjust `max_workers` in `run_grading_streaming` (line 956)

### Issue: Session state issues with variable questions
- **Cause**: Old session state from previous assignment
- **Solution**: Clear browser cache or use incognito mode

## Summary

The system now fully supports **1-25 variable questions** per assignment with:
- ‚úÖ Dynamic question rendering
- ‚úÖ Parallel grading (up to 10 concurrent threads)
- ‚úÖ Adaptive UI based on question count
- ‚úÖ Full backward compatibility with 3-question assignments
- ‚úÖ Session restoration with variable questions
- ‚úÖ Retry functionality with variable questions
- ‚úÖ Memory management for variable questions

